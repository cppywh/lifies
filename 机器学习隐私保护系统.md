# 论文名字

## 预备知识

### 线性回归(Linear regression)

给定 n 个训练数据样本 \(\mathbf{x}_i\)，每个样本包含 d 个特征，以及对应的输出标签 \(y_i\)。线性回归是一种学习函数 g 的统计过程，使得 \(g(\mathbf{x}_i) \approx y_i\)。

------

在线性回归中，函数 g 被假设为**线性的**，可表示为 \(\mathbf{x}_i\) 与系数向量 \(\mathbf{w}\) 的内积：

\(g(\mathbf{x}_i) = \sum_{j=1}^d x_{ij} w_j = \mathbf{x}_i \cdot \mathbf{w}\)

------

为了学习系数向量 \(\mathbf{w}\)，需要定义损失函数 \(C(\mathbf{w})\)，并通过优化 \(argmin_{\mathbf{w}} C(\mathbf{w})\) 来**求解 \(\mathbf{w}\)**。线性回归中常用的损失函数为：

\(C(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^n C_i(\mathbf{w})\)

其中 \(C_i(\mathbf{w}) = \frac{1}{2} (\mathbf{x}_i \cdot \mathbf{w} - y_i)^2\)。

------

该优化问题的解可通过求解线性方程组得到：

\((\mathbf{X}^T \mathbf{X}) \times \mathbf{w} = \mathbf{X}^T \mathbf{Y}\)

其中 \(\mathbf{X}\) 是表示所有输入数据的 \(n \times d\) 矩阵，\(\mathbf{Y}\) 是表示输出标签的 \(n \times 1\) 矩阵。

不过，计算\(\mathbf{X}^T \mathbf{X}\)的实际复杂度：\(O(n \cdot d^2)\)，当得到\(\mathbf{X}^T \mathbf{X}\)（\(d \times d\)矩阵）后，需要求解线性方程组\((\mathbf{X}^T \mathbf{X}) \mathbf{w} = \mathbf{X}^T \mathbf{Y}\)。此时若用**矩阵求逆法**（\(\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}\)），\(d \times d\)矩阵的求逆复杂度是 \(O(d^3)\)。由于**复杂度较高**，除了 n 和 d 较小的情况外，这种方法在实际中很少使用。



### 随机梯度下降 Stochastic gradient descent (SGD)

SGD 是一种**逐步逼近函数局部最小值**的有效近似算法。对于前文描述的线性回归问题：由于其优化函数是凸函数，SGD 可以收敛到全局最小值，且在实际应用中速度通常很快。

此外，SGD 还能推广应用于**逻辑回归、神经网络训练**等场景 —— 这些任务对应的优化问题没有闭式解，因此 SGD 是实际中训练这类模型最常用的方法，也是本文的核心研究重点。

**SGD 的工作流程**

1. **初始化系数向量**：将\(\mathbf{w}\)初始化为随机值向量或全 0 向量。

2. **迭代更新**：每次迭代时，随机选取一个样本\((\mathbf{x}_i, y_i)\)，并按以下公式更新系数\(w_j\)：

   \(w_j := w_j - \alpha \frac{\partial C_i(\mathbf{w})}{\partial w_j} \tag{1}\)

   \(\alpha\)是学习率，用于定义每次迭代向最小值方向移动的步长。

将线性回归的损失函数代入后，更新公式可简化为：

\(w_j := w_j - \alpha (\mathbf{x}_i \cdot \mathbf{w} - y_i) x_{ij}\)

- 计算预测输出\(\hat{y}_i = \mathbf{x}_i \cdot \mathbf{w}\)的过程称为**前向传播**；
- 计算变化量\(\alpha (\mathbf{x}_i \cdot \mathbf{w} - y_i) x_{ij}\)的过程称为**反向传播**。



**图**包含两部分内容：

- (a) 逻辑函数：横坐标为u，纵坐标为逻辑函数\(f(u)\)，函数曲线呈现 “S 型”（在\(u \approx -5\)到0区间快速上升，两端趋于平稳）。
- (b) 神经网络示例：结构包含 4 层 —— 输入层（节点为\(x_1, x_2, \dots, x_d\)）、隐藏层 1、隐藏层\(m-1\)、输出层（节点为\(y_1, y_2, \dots, y_k\)）；每层节点均通过激活函数f连接到下一层节点。

SGD是一种**逐步逼近函数局部最小值**的有效近似算法。对于前文描述的线性回归问题：由于其优化函数是凸函数，SGD 可以收敛到全局最小值，且在实际应用中速度通常很快。

此外，SGD 还能推广应用于**逻辑回归、神经网络训练**等场景 —— 这些任务对应的优化问题没有闭式解，因此 SGD 是实际中训练这类模型最常用的方法，也是本文的核心研究重点。

**SGD 的工作流程**

1. **初始化系数向量**：将\(\mathbf{w}\)初始化为随机值向量或全0向量。

2. **迭代更新**：每次迭代时，随机选取一个样本\((\mathbf{x}_i, y_i)\)，并按以下公式更新系数\(w_j\)：

   \(w_j := w_j - \alpha \frac{\partial C_i(\mathbf{w})}{\partial w_j} \tag{1}\)

   其中，**\(\alpha\)**是学习率，用于定义每次迭代向最小值方向移动的步长。然后后面那个是**求偏导**

   将线性回归的损失函数代入后：注意计算

   \(w_j := w_j - \alpha (\mathbf{x}_i \cdot \mathbf{w} - y_i) x_{ij}\)

   - 计算预测输出\(y_i^* = \mathbf{x}_i \cdot \mathbf{w}\)的过程称为**前向传播**；
   - 计算变化量\(\alpha (y_i^* - y_i) x_{ij}\)的过程称为**反向传播**。

![image-20251126111738470](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20251126111738470.png)

**图**包含两部分内容：

- (a) 逻辑函数：横坐标为u，纵坐标为逻辑函数\(f(u)\)，函数曲线呈现 “S 型”（在\(u \approx -5\)到0区间快速上升，两端趋于平稳）。
- (b) 神经网络示例：结构包含 —— 输入层（节点为\(x_1, x_2, \dots, x_d\)）、隐藏层 1、...、隐藏层\(m-1\)、输出层（节点为\(y_1, y_2, \dots, y_k\)）；每层节点均通过激活函数f连接到下一层节点。



### 小批量梯度下降（Mini-batch SGD）

在实际应用中，我们不会在每次迭代时只选择**一个样本**，而是随机选取**一小批样本**，通过这批样本偏导数的平均值来更新参数\(\mathbf{w}\)。

我们用集合**B**表示这一批样本的索引，这种方法被称为**小批量 SGD**；其中**\(|B|\)**代表小批量的大小（通常取值范围是 2 到 200）。

**小批量 SGD 的优势**

1. **计算加速**：可以借助向量化计算库来加速运算，相比无小批量的\(|B|\)次迭代，小批量的单次迭代速度要快得多；
2. **收敛更优**：小批量能让\(\mathbf{w}\)向最小值收敛的过程更平滑、更快速。

小批量下的参数更新函数可以表示为向量化形式：

\(\mathbf{w} := \mathbf{w} - \frac{1}{|B|} \alpha \mathbf{X}_B^T \left( \mathbf{X}_B \times \mathbf{w} - \mathbf{Y}_B \right) \tag{2}\)

- \(\mathbf{X}_B\)和\(\mathbf{Y}_B\)分别是从\(\mathbf{X}\)和\(\mathbf{Y}\)中，通过索引集合B选取的\(B \times d\)子矩阵、\(B \times 1\)子矩阵，对应迭代中这\(|B|\)个样本的数据与标签；

- 此处的\(\mathbf{w}\)被视为列向量×d。

  

### 学习率调整

若学习率\(\alpha\)设置过大，SGD 的结果可能会发散（无法收敛到最小值）。因此，我们会借助**测试集**来调整学习率，具体流程如下：

1. 计算测试集中每个样本与当前\(\mathbf{w}\)的内积，作为预测结果；
2. 将预测结果与对应标签对比，得到测试集上的**准确率**（正确预测的样本占比）。

若测试集准确率下降，则降低学习率，并使用新的学习率重新启动训练。

为了平衡测试过程的开销，实际中通常遵循以下操作：

- 先打乱所有训练样本；
- 每次迭代选取一个小批量样本训练；
- 按顺序用完所有训练样本（该过程称为一个轮次**epoch**）后，再计算测试集准确率；
- 若准确率下降：将学习率减半，重新开始训练；
- 若准确率未下降：继续训练，待训练样本用尽后，再次打乱样本并启动下一个 epoch。

### 终止条件

当当前轮次的准确率与前一个轮次的准确率差异**低于一个极小的阈值**时，认为\(\mathbf{w}\)已收敛到最小值，算法终止。

**变量关系说明**

我们定义：

- n：训练样本总数
- E：训练的轮次数
- \(|B|\)：小批量大小
- t：总迭代次数

这些变量满足如下关系：其实就是用了多少次$x_i$

\(n \cdot E = |B| \cdot t\)



